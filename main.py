import threading
import queue
import time
import wave
import subprocess
import pyaudio
import openai
import serial
import re

# ============ è¨­å®š ============
PORT = "/dev/cu.usbserial-140"
BAUD = 115200
DT = 0.3
THRESH_PITCH = 20
THRESH_YAW = 25
COOLDOWN = 10.0

BUFFER_SECONDS = 8   # â˜… ç›´è¿‘8ç§’ã‚’ä¿æŒ
RATE = 16000
CHUNK = 1024
MIN_SEC = 0.5

client = openai.OpenAI()

is_playing_audio = False   # å†ç”Ÿä¸­ã‹ã©ã†ã‹
is_processing_reaction = False  # ç›¸æ§Œå‡¦ç†ä¸­ã‹ã©ã†ã‹
# =====================================


# =====================================
# â˜… 1. ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªéŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰ï¼ˆå¸¸æ™‚éŒ²éŸ³ â†’ å¾ªç’°ãƒãƒƒãƒ•ã‚¡ã¸ï¼‰
# =====================================
audio_q = queue.Queue(maxsize=int(BUFFER_SECONDS * RATE / CHUNK))

def audio_record_loop():
    global is_playing_audio

    pa = pyaudio.PyAudio()
    stream = pa.open(
        format=pyaudio.paInt16,
        channels=1,
        rate=RATE,
        input=True,
        frames_per_buffer=CHUNK
    )

    print("ğŸ¤ Audio recording started...")

    while True:
        data = stream.read(CHUNK)

        # â˜… å†ç”Ÿä¸­ã¯éŒ²éŸ³ã¯ã™ã‚‹ãŒãƒãƒƒãƒ•ã‚¡ã«ã¯è©°ã‚ãªã„
        if is_playing_audio:
            continue

        if audio_q.full():
            audio_q.get()  # å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’æ¨ã¦ã‚‹
        audio_q.put(data)


# éŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰èµ·å‹•
threading.Thread(target=audio_record_loop, daemon=True).start()


# =====================================
# â˜… 2. ãƒãƒƒãƒ•ã‚¡å†…å®¹ã‚’ WAV ã¨ã—ã¦ä¿å­˜
# =====================================
def save_buffer_to_wav(path="input.wav"):
    frames = list(audio_q.queue)  # ç¾åœ¨ã®ãƒãƒƒãƒ•ã‚¡ã‚’ã‚³ãƒ”ãƒ¼
    # 0.5ç§’åˆ†ä»¥ä¸Šã®ãƒãƒ£ãƒ³ã‚¯ãŒãªã‘ã‚Œã°è«¦ã‚ã‚‹
    if len(frames) * CHUNK < RATE * MIN_SEC:
        return False

    # â˜… ã“ã“ã§ç©ºãƒã‚§ãƒƒã‚¯
    if not frames:   # len(frames) == 0 ã¨åŒã˜
        return False

    wf = wave.open(path, 'wb')
    wf.setnchannels(1)
    wf.setsampwidth(2)
    wf.setframerate(RATE)
    wf.writeframes(b''.join(frames))
    wf.close()

    return True



# =====================================
# â˜… 3. Whisper ã§æ–‡å­—èµ·ã“ã—
# =====================================
def speech_to_text(filename="input.wav"):
    with open(filename, "rb") as f:
        res = client.audio.transcriptions.create(
            model="gpt-4o-transcribe",
            file=f
        )
    return res.text


# =====================================
# â˜… 4. ç›¸æ§Œç”Ÿæˆï¼ˆè‚¯å®š / å¦å®šã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆ†å²ï¼‰
# =====================================
def generate_backchannel(user_text, motion_type):
    """
    motion_type:
        "nod"   -> è‚¯å®šã®ç›¸æ§Œ
        "shake" -> å¦å®šã®ç›¸æ§Œ
    """

    if motion_type == "nod":
        # é ·ãï¼šè‚¯å®šãƒ»å…±æ„Ÿç³»ã®ç›¸æ§Œ
        prompt = f"""ã‚ãªãŸã¯ä¼šè©±ç›¸æ§ŒAIã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ã®ç™ºè©±å†…å®¹ã‚’ç†è§£ã—ãŸã†ãˆã§ã€è‚¯å®šãƒ»å…±æ„Ÿãƒ»åŒæ„ã‚’ç¤ºã™è‡ªç„¶ãªçŸ­ã„æ—¥æœ¬èªã®ç›¸æ§Œã‚’è¿”ã—ã¦ãã ã•ã„ã€‚

- è¿”ç­”ã¯äºŒæ–‡ä»¥å†…ã€20ã€œ30æ–‡å­—ãã‚‰ã„ã¾ã§ã«ã—ã¦ãã ã•ã„ã€‚
- æ•¬èª / ã§ã™ã¾ã™èª¿ã‚’åŸºæœ¬ã¨ã—ã¦ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ã®ç™ºè©±å†…å®¹ã‚’è¸ã¾ãˆãŸè¿”ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶: {user_text}
ç›¸æ§Œï¼š"""
    elif motion_type == "shake":
        # é¦–æŒ¯ã‚Šï¼šå¦å®šãƒ»é•ã„ã‚’ç©ã‚„ã‹ã«å—ã‘æ­¢ã‚ã‚‹ç›¸æ§Œ
        prompt = f"""ã‚ãªãŸã¯ä¼šè©±ç›¸æ§ŒAIã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ã®ç™ºè©±å†…å®¹ã‚’ç†è§£ã—ãŸã†ãˆã§ã€ã€Œå¦å®šã€ã‚„ã€Œãã†ã§ã¯ãªã„ã€ã¨ã„ã†ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã‚’ç©ã‚„ã‹ã«å—ã‘æ­¢ã‚ã‚‹è‡ªç„¶ãªçŸ­ã„æ—¥æœ¬èªã®ç›¸æ§Œã‚’è¿”ã—ã¦ãã ã•ã„ã€‚

- ç›¸æ‰‹ã‚’è²¬ã‚ãŸã‚Šå¼·ãå¦å®šã—ãŸã‚Šã™ã‚‹è¡¨ç¾ã¯é¿ã‘ã¦ãã ã•ã„ã€‚
- è¿”ç­”ã¯äºŒæ–‡ä»¥å†…ã€20ã€œ30æ–‡å­—ãã‚‰ã„ã¾ã§ã«ã—ã¦ãã ã•ã„ã€‚
- æ•¬èª / ã§ã™ã¾ã™èª¿ã‚’åŸºæœ¬ã¨ã—ã¦ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ã®ç™ºè©±å†…å®¹ã‚’è¸ã¾ãˆãŸè¿”ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶: {user_text}
ç›¸æ§Œï¼š"""
    else:
        # ä¿é™ºï¼šå¾“æ¥ã¨åŒã˜ç„¡é›£ãªç›¸æ§Œ
        prompt = f"ãƒ¦ãƒ¼ã‚¶ã®ç™ºè©±ã«è‡ªç„¶ã§çŸ­ã„ç›¸æ§Œã‚’è¿”ã—ã¦ãã ã•ã„ã€‚\nãƒ¦ãƒ¼ã‚¶:{user_text}\nç›¸æ§Œ:"

    res = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )
    return res.choices[0].message.content


# =====================================
# â˜… 5. éŸ³å£°ç”Ÿæˆï¼ˆTTSï¼‰
# =====================================
def text_to_wav(text, out_path="reply.wav"):
    res = client.audio.speech.create(
        model="gpt-4o-mini-tts",
        voice="alloy",
        input=text,
        response_format="wav"
    )
    data = res.read()
    with open(out_path, "wb") as f:
        f.write(data)


# =====================================
# â˜… 6. éŸ³å£°å†ç”Ÿï¼ˆafplayï¼‰
# =====================================
def clear_audio_buffer():
    while not audio_q.empty():
        try:
            audio_q.get_nowait()
        except queue.Empty:
            break

def play_audio(path="reply.wav"):
    global is_playing_audio
    is_playing_audio = True
    try:
        subprocess.run(["afplay", path])
    finally:
        is_playing_audio = False
        clear_audio_buffer()



# =====================================
# â˜… 7. é ·ããƒ»é¦–æŒ¯ã‚Šæ¤œå‡ºï¼ˆreaction_detect ã‚’çµ±åˆï¼‰
# =====================================
def parse_line(line):
    vals = re.findall(r"[-+]?\d*\.\d+|\d+", line)
    if len(vals) >= 6:
        return tuple(map(float, vals[:6]))
    return None


def head_motion_loop():
    global is_processing_reaction

    ser = serial.Serial(PORT, BAUD, timeout=1)
    print("ğŸ” Listening for head motion...")

    pitch_angle = 0.0
    yaw_angle = 0.0
    last_detect = 0.0

    while True:
        line = ser.readline().decode(errors='ignore').strip()
        vals = parse_line(line)
        if not vals:
            continue

        # â˜… å‡¦ç†ä¸­ã¯ç©åˆ†ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ã‚¹ã‚­ãƒƒãƒ—
        if is_processing_reaction:
            pitch_angle = yaw_angle = 0.0
            continue

        ax, ay, az, gx, gy, gz = vals

        pitch_angle += gy * DT
        yaw_angle += gz * DT
        pitch_angle *= 0.98
        yaw_angle *= 0.98

        now = time.time()

        if abs(pitch_angle) > THRESH_PITCH and (now - last_detect > COOLDOWN):
            print("ğŸ”´ SHAKE detected!")
            last_detect = now
            handle_reaction("shake")
            pitch_angle = yaw_angle = 0.0

        elif abs(yaw_angle) > THRESH_YAW and (now - last_detect > COOLDOWN):
            print("ğŸŸ¢ NOD detected!")
            last_detect = now
            handle_reaction("nod")
            pitch_angle = yaw_angle = 0.0



# =====================================
# â˜… 8. åå¿œå‡¦ç†ï¼ˆéŒ²éŸ³â†’ãƒ†ã‚­ã‚¹ãƒˆâ†’ç›¸æ§Œâ†’éŸ³å£°â†’å†ç”Ÿï¼‰
# =====================================
def handle_reaction(motion_type):
    global is_processing_reaction

    if is_processing_reaction:
        print("â³ Reaction already in progress. Ignored.")
        return

    is_processing_reaction = True
    try:
        print("ğŸ’¾ Saving audio buffer...")
        ok = save_buffer_to_wav("input.wav")
        if not ok:
            print("âš ï¸ Audio buffer is empty. Skipping transcription.")
            return

        print("ğŸ“ Transcribing...")
        text = speech_to_text("input.wav")
        print(f"You said: {text}")

        if len(text.strip()) == 0:
            print("âš ï¸ No speech detected.")
            return

        print("ğŸ’¬ Generating backchannel...")
        reply = generate_backchannel(text, motion_type)
        print(f"Backchannel ({motion_type}): {reply}")

        print("ğŸ”Š Generating audio...")
        text_to_wav(reply, "reply.wav")

        print("â–¶ï¸ Playing...")
        play_audio("reply.wav")

        print("--------------------------------")
    finally:
        is_processing_reaction = False




# =====================================
# â˜… ãƒ¡ã‚¤ãƒ³ï¼šé ­ã®å‹•ãç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰èµ·å‹•
# =====================================
threading.Thread(target=head_motion_loop, daemon=True).start()

print("\nğŸ§ è©±ã—ãªãŒã‚‰é ·ã / é¦–ã‚’æŒ¯ã‚‹ã¨ç›¸æ§ŒãŒè¿”ã£ã¦ãã¾ã™ï¼\n")

# ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã¯å¾…æ©Ÿ
while True:
    time.sleep(1)
